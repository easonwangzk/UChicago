{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMNQ04t0z15woNIXBmdOC4W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/easonwangzk/UChicago/blob/main/Skip_gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "import requests\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter, OrderedDict\n",
        "from itertools import chain\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "0gTrfO1Grc0n"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Device setup"
      ],
      "metadata": {
        "id": "Khj_Sy7Mr5Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} as device.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q7kTaujr8nL",
        "outputId": "5481db4e-92e9-4b5b-c43d-4df819678660"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda as device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configs"
      ],
      "metadata": {
        "id": "WPyPU2egr_m_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"./data\"\n",
        "model_dir = \"./models\"\n",
        "debug = True\n",
        "\n",
        "if debug:\n",
        "    CONTEXT_WINDOW = 2\n",
        "    EMBEDDING_SIZE = 5\n",
        "    MIN_FREQ = 5\n",
        "    BATCH_SIZE = 3\n",
        "    N_EPOCHS = 1\n",
        "else:\n",
        "    CONTEXT_WINDOW = 4\n",
        "    EMBEDDING_SIZE = 100\n",
        "    MIN_FREQ = 25\n",
        "    BATCH_SIZE = 64\n",
        "    N_EPOCHS = 3"
      ],
      "metadata": {
        "id": "TkfrEtnysDEe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create dirs"
      ],
      "metadata": {
        "id": "J5GwXnhWsSIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(data_dir, exist_ok=True)\n",
        "os.makedirs(model_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "PGFbWq1ssVlu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and tokenize text"
      ],
      "metadata": {
        "id": "UFa7y2nhsYD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.gutenberg.org/cache/epub/7370/pg7370.txt\"\n",
        "response = requests.get(url)\n",
        "raw_text = response.text.lower()\n",
        "raw_text = re.sub(r'[^a-z\\s]', '', raw_text)\n",
        "sentences = [line.split() for line in raw_text.split('\\n') if line.strip()]\n",
        "print(f\"Number of sentences: {len(sentences):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DussqJ0MsagF",
        "outputId": "3d9ec680-dcd4-47da-d2cc-a0353c615d1d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 4,957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Vocabulary class (same as CBOW version)"
      ],
      "metadata": {
        "id": "G6e44DWrsc9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    def __init__(self, word_counts: OrderedDict, min_freq: int = 1, max_size: int = None, specials: List[str] = None, unk_token: str = \"<unk>\"):\n",
        "        self.word_counts = word_counts\n",
        "        self.min_freq = min_freq\n",
        "        self.max_size = max_size\n",
        "        self.unk_token = unk_token\n",
        "        self.specials = list(specials) if specials else []\n",
        "\n",
        "        if self.unk_token not in self.specials:\n",
        "            self.specials.insert(0, self.unk_token)\n",
        "\n",
        "        self.token2idx = {}\n",
        "        self.idx2token = []\n",
        "        self._prepare_vocab()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2token)\n",
        "\n",
        "    def __contains__(self, value):\n",
        "        return value in self.idx2token\n",
        "\n",
        "    def _prepare_vocab(self):\n",
        "        vocab_list = self.specials.copy()\n",
        "        filtered_words = [word for word, freq in self.word_counts.items() if freq >= self.min_freq and word not in self.specials]\n",
        "        if self.max_size is not None:\n",
        "            filtered_words = filtered_words[:self.max_size - len(self.specials)]\n",
        "        vocab_list.extend(filtered_words)\n",
        "        self.idx2token = vocab_list\n",
        "        self.token2idx = {word: idx for idx, word in enumerate(vocab_list)}\n",
        "\n",
        "    def get_token(self, idx: int) -> str:\n",
        "        return self.idx2token[idx] if 0 <= idx < len(self.idx2token) else self.unk_token\n",
        "\n",
        "    def get_index(self, token: str) -> int:\n",
        "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
        "\n",
        "    def get_tokens(self, indices: List[int]) -> List[str]:\n",
        "        return [self.get_token(idx) for idx in indices]\n",
        "\n",
        "    def get_indices(self, tokens: List[str]) -> List[int]:\n",
        "        return [self.get_index(token) for token in tokens]"
      ],
      "metadata": {
        "id": "x9gV2NoasfnN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding for skip-gram"
      ],
      "metadata": {
        "id": "PES9ALTWsmXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sentences(sentences: List[List[str]], context_length: int, pad_token: str = \"<pad>\") -> List[List[str]]:\n",
        "    padded_sentences = []\n",
        "    for sentence in sentences:\n",
        "        padded_sentence = [pad_token] * context_length + sentence + [pad_token] * context_length\n",
        "        padded_sentences.append(padded_sentence)\n",
        "    return padded_sentences"
      ],
      "metadata": {
        "id": "z6Cuo7aispl7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = pad_sentences(sentences, CONTEXT_WINDOW)"
      ],
      "metadata": {
        "id": "6bunxvYPs0w5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build vocab"
      ],
      "metadata": {
        "id": "OqfQctNksswi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocab(OrderedDict(Counter(chain.from_iterable(sentences))), min_freq=MIN_FREQ, specials=[\"<pad>\"])\n",
        "print(f\"Size of Vocabulary: {len(vocab):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nPBlky7s4D1",
        "outputId": "cedabe98-c965-4814-e8a2-77ac4a2ce821"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Vocabulary: 1,158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Skip-gram pair generation"
      ],
      "metadata": {
        "id": "xAXtdfags6ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_skipgram_pairs(sentences: List[List[str]], context_length: int, vocab: Vocab):\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "    for sentence in sentences:\n",
        "        encoded = vocab.get_indices(sentence)\n",
        "        for center_idx in range(context_length, len(encoded) - context_length):\n",
        "            center_word = encoded[center_idx]\n",
        "            context = encoded[center_idx - context_length : center_idx] + encoded[center_idx + 1 : center_idx + context_length + 1]\n",
        "            for context_word in context:\n",
        "                inputs.append(center_word)\n",
        "                outputs.append(context_word)\n",
        "    return torch.tensor(inputs), torch.tensor(outputs)\n",
        "\n",
        "inputs, outputs = generate_skipgram_pairs(sentences, CONTEXT_WINDOW, vocab)\n",
        "print(f\"Number of training examples: {len(inputs):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x51_SV14s9yY",
        "outputId": "72e5b9db-71a2-4192-858b-dd553829b95d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 236,704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset class"
      ],
      "metadata": {
        "id": "lqnhGseutA1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramDataset(Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]"
      ],
      "metadata": {
        "id": "0HG8k7KmtDG-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skip-gram model"
      ],
      "metadata": {
        "id": "UmZDRhZStFc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, center_words):\n",
        "        embeds = self.embeddings(center_words)\n",
        "        out = self.linear(embeds)\n",
        "        return out\n",
        "\n",
        "    def debug_forward(self, center_words):\n",
        "        embeds = self.embeddings(center_words)\n",
        "        print(\"\\nembeddings shape:\", embeds.shape)\n",
        "        print(embeds)\n",
        "        out = self.linear(embeds)\n",
        "        print(\"\\nlogits shape:\", out.shape)\n",
        "        print(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "hGxDDaIUtFL2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate model"
      ],
      "metadata": {
        "id": "XmXRsNBStNnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SkipGram(vocab_size=len(vocab), embedding_dim=EMBEDDING_SIZE).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2-3V0setNZK",
        "outputId": "61630f59-9743-47d6-bb91-668785ae2256"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SkipGram(\n",
            "  (embeddings): Embedding(1158, 5)\n",
            "  (linear): Linear(in_features=5, out_features=1158, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss and optimizer"
      ],
      "metadata": {
        "id": "9CofeFdWtTGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab.get_index(vocab.unk_token))\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "RE_giWxqtV_S"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader"
      ],
      "metadata": {
        "id": "Wa0WOIABtZnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = SkipGramDataset(inputs, outputs)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "356_6QCTtb1U"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "3xJ6qhMUteOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "    total_loss = 0\n",
        "    for batch_inputs, batch_outputs in dataloader:\n",
        "        batch_inputs, batch_outputs = batch_inputs.to(device), batch_outputs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        if debug:\n",
        "            predictions = model.debug_forward(batch_inputs)\n",
        "        else:\n",
        "            predictions = model.forward(batch_inputs)\n",
        "\n",
        "        loss = criterion(predictions, batch_outputs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if debug: break\n",
        "    if debug: break\n",
        "    print(f\"Epoch {epoch+1}/{N_EPOCHS}, Loss: {total_loss/len(dataset):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dh1QxsmotgQK",
        "outputId": "e6ed8cb9-adc9-4ea0-a0d8-2960f7401f48"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "embeddings shape: torch.Size([3, 5])\n",
            "tensor([[ 0.0099,  0.8007, -0.2172, -1.7865, -0.1345],\n",
            "        [-0.1325, -1.2426, -0.1149,  1.1431,  0.3546],\n",
            "        [-2.8135,  0.0679,  0.0196, -0.9808,  0.5849]], device='cuda:0',\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "logits shape: torch.Size([3, 1158])\n",
            "tensor([[ 0.6994,  0.6161,  0.1455,  ...,  0.0060,  0.0517,  0.4258],\n",
            "        [-0.0163,  0.1403, -0.6382,  ..., -0.2566, -0.6915, -0.8407],\n",
            "        [-0.2563, -0.1448, -0.0099,  ..., -0.1927,  0.5144,  0.6950]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save trained model weights and vocab"
      ],
      "metadata": {
        "id": "o-0K3JTiu3_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import pickle"
      ],
      "metadata": {
        "id": "fKQPx6ODu_Ar"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.embeddings.weight.data, f\"{model_dir}/weights.pt\")\n",
        "with open(f\"{model_dir}/vocab.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vocab, f)"
      ],
      "metadata": {
        "id": "Bo4Pzav2u8hj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Define function to compute closest words"
      ],
      "metadata": {
        "id": "2BNSog-9vBrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def closest_words(embeddings, vocab, word, n=10):\n",
        "    if word not in vocab.token2idx:\n",
        "        raise ValueError(f\"'{word}' not in vocabulary\")\n",
        "\n",
        "    word_idx = vocab.get_index(word)\n",
        "    word_embedding = embeddings[word_idx]\n",
        "\n",
        "    similarities = F.cosine_similarity(word_embedding.unsqueeze(0), embeddings, dim=1)\n",
        "    similarities[word_idx] = -1  # exclude itself\n",
        "\n",
        "    top_indices = similarities.topk(n).indices\n",
        "    return [(vocab.get_token(idx), similarities[idx].item()) for idx in top_indices]"
      ],
      "metadata": {
        "id": "Fjt94vcavFkG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    loaded_embeddings = torch.load(f\"{model_dir}/weights.pt\", weights_only=True)\n",
        "else:\n",
        "    loaded_embeddings = torch.load(f\"{model_dir}/weights.pt\", weights_only=True, map_location=torch.device(\"cpu\"))\n",
        "\n",
        "with open(f\"{model_dir}/vocab.pkl\", \"rb\") as f:\n",
        "    loaded_vocab = pickle.load(f)"
      ],
      "metadata": {
        "id": "l3K0NsZKvOeQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run similarity search"
      ],
      "metadata": {
        "id": "FtVJ112bvTtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Trained model:\")\n",
        "print(closest_words(embeddings=loaded_embeddings, vocab=loaded_vocab, word=\"love\", n=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMRGNCodvX9h",
        "outputId": "cf846faa-9b2f-473e-fc9d-4fd1850d8b8a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained model:\n",
            "[('was', 0.9637019038200378), ('make', 0.905871570110321), ('judge', 0.9049926996231079), ('body', 0.9028736352920532), ('brought', 0.8975038528442383), ('minority', 0.8772290945053101), ('wise', 0.8752244710922241), ('kingdom', 0.8742192387580872), ('grown', 0.8639228940010071), ('food', 0.8584514856338501)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Compare with untrained model"
      ],
      "metadata": {
        "id": "9g12zcLpviU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_untrained = SkipGram(vocab_size=len(vocab), embedding_dim=EMBEDDING_SIZE)\n",
        "untrained_embeddings = model_untrained.embeddings.weight.data\n",
        "\n",
        "print(\"\\nUntrained model:\")\n",
        "print(closest_words(embeddings=untrained_embeddings, vocab=vocab, word=\"love\", n=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQR_iEdevgGX",
        "outputId": "d9d3ef2b-d17b-4533-8b9e-817c4fd29ca9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Untrained model:\n",
            "[('or', 0.9682488441467285), ('draw', 0.9493830800056458), ('agreed', 0.9472517371177673), ('paragraph', 0.9458328485488892), ('many', 0.9302278161048889), ('grants', 0.928244411945343), ('measure', 0.9108309149742126), ('understood', 0.9096970558166504), ('freemen', 0.9056882262229919), ('power', 0.89534991979599)]\n"
          ]
        }
      ]
    }
  ]
}